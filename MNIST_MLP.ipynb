{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required modules\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Now what is that one_hot = True?\\n One-hot-encoding uses a vector of binary values to represent numeric or categorical values. \\n As our labels are for the digits 0-9, the vector contains ten values, one for each possible digit. \\n One of these values is set to 1, to represent the digit at that index of the vector, and the rest are set to 0. \\n For example, the digit 3 is represented using the vector [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. As the value at index 3 is stored as 1, the vector therefore represents the digit 3'"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "\n",
    "\"\"\"Now what is that one_hot = True?\n",
    " One-hot-encoding uses a vector of binary values to represent numeric or categorical values. \n",
    " As our labels are for the digits 0-9, the vector contains ten values, one for each possible digit. \n",
    " One of these values is set to 1, to represent the digit at that index of the vector, and the rest are set to 0. \n",
    " For example, the digit 3 is represented using the vector [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. As the value at index 3 is stored as 1, the vector therefore represents the digit 3\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print (mnist.train.num_examples)\n",
    "print (mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hyper parameters (learning rate, training epochs, batch size, display step)\n",
    "\n",
    "\"\"\"Let me first understand what each of the parameters mean\n",
    "\n",
    "Learning Rate:\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "Step size = Slope X Learning Rate\n",
    "Practically, Gradient Descent stops when Slope is very close to 0 i.e 0.0001 so if slope is 0.009 X 0.001 = 0.00009 Which is smaller than our learning rate, so we stop\n",
    "\n",
    "Way the learning rate changes from relatively large to relatively small is called the SCHEDULE\n",
    "Basically Sum of squared residuals mei kitne se jump maarega\n",
    "Deep learning neural networks are trained using the stochastic gradient descent algorithm.\n",
    "(Stochastic GD randomly picks up one sample for each step and just use that one sample to calculate the derivates, Thus it reduces the calculations by a factor of n where n is the number of samples)\n",
    "Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.\n",
    "The amount that the weights are updated during training is referred to as the step size or the “learning rate.\n",
    "A neural network learns or approximates a function to best map inputs to outputs from examples in the training dataset.\n",
    "The learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated, such as at the end of each batch of training examples.\n",
    "Given a perfectly configured learning rate, the model will learn to best approximate the function given available resources (the number of layers and the number of nodes per layer) in a given number of training epochs (passes through the training data).\n",
    "Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.\n",
    "At extremes, a learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs. Oscillating performance is said to be caused by weights that diverge (are divergent). A learning rate that is too small may never converge or may get stuck on a suboptimal solution.\n",
    "\n",
    "For Big Data, Gradient Descent is slow, this is where stochastic Gradient descent comes in handy\n",
    "\n",
    "Training Epochs:\n",
    "~~~~~~~~~~~~~~~~\n",
    "The number of iterations to train the Stochastic Gradient Descent\n",
    "\n",
    "Batch Size\n",
    "~~~~~~~~~~~\n",
    "The strict definition of Stochastic Gradient Descent is to take one sample each step.\n",
    "However, it is more common to select mini batch for each step. Thus this parameter becomes the batch size.\n",
    "”\"\"\"\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100   #Good practice to set higher\n",
    "batch_size = 256\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup network parameters (size of different hidden layers, size of input features, size of output classes)\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_hidden_3 = 256\n",
    "\n",
    "\"\"\"Since input is of 28x28 images, we have total 784 pixel values\"\"\"\n",
    "n_input = 784 \n",
    "\n",
    "\"\"\"Each of the image can only belong to a class from 0 to 9\"\"\"\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input placeholders for X with shape of (None initial, n_input ) and Y with shape (None initial , n_classes )\n",
    "\n",
    "\"\"\"To build our network, we will set up the network as a computational graph for TensorFlow to execute. \n",
    "The core concept of TensorFlow is the tensor, a data structure similar to an array or list. \n",
    "initialized, manipulated as they are passed through the graph, and updated through the learning process.\n",
    "\n",
    "We’ll start by defining tensors as placeholders, which are tensors that we'll feed values into later. Add the following to your file:\n",
    "The only parameter that needs to be specified at its declaration is the size of the data we will be feeding in. \n",
    "For X we use a shape of [None, 784], where None represents any amount, as we will be feeding in an undefined number of 784-pixel images. \n",
    "The shape of Y is [None, 10] as we will be using it for an undefined number of label outputs, with 10 possible classes. \n",
    "\"\"\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup weight and bias variables\n",
    "\"\"\"\n",
    "The parameters that the network will update in the training process are the weight and bias values, so for these we need to set an initial value rather than an empty placeholder. \n",
    "These values are essentially where the network does its learning, as they are used in the activation functions of the neurons, representing the strength of the connections between units.\n",
    "\n",
    "Since the values are optimized during training, we could set them to zero for now. But the initial value actually has a significant impact on the final accuracy of the model. \n",
    "We'll use random values from a truncated normal distribution for the weights. \n",
    "We want them to be close to zero, so they can adjust in either a positive or negative direction, and slightly different, so they generate different errors. \n",
    "This will ensure that the model learns something useful\n",
    "\n",
    "\"\"\"\n",
    "W = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='h1'),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='h2'),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3]), name='h3'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]), name='w_out')\n",
    "}\n",
    "\n",
    "b = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3]), name='b3'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='b_out')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function name multilayer_perceptron(x,weights, biases) and return out_layer\n",
    "\n",
    "def multilayer_perceptron(x, weights, bias):\n",
    "    # 1 hidden layer with Linear and then RELU Activation\n",
    "    layer1 = tf.add(tf.matmul(x, weights['h1']), bias['b1'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    # 2 hidden layer with Linear and then RELU Activation\n",
    "    layer2 = tf.add(tf.matmul(layer1, weights['h2']), bias['b2'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    layer3 = tf.add(tf.matmul(layer1, weights['h3']), bias['b3'])\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "\n",
    "    \n",
    "    # Outut layer with Linear Activation\n",
    "    out_layer = tf.add(tf.matmul(layer3, weights['out']), bias['out'])\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pred model\n",
    "pred = multilayer_perceptron(X, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup cost which is average of cross entropy function aka -ylog(pred)\n",
    "\"\"\"The final step in building the graph is to define the loss function that we want to optimize. \n",
    "A popular choice of loss function in TensorFlow programs is cross-entropy, also known as log-loss, which quantifies the difference between two probability distributions (the predictions and the labels). \n",
    "A perfect classification would result in a cross-entropy of 0, with the loss completely minimized.\n",
    "\n",
    "We also need to choose the optimization algorithm which will be used to minimize the loss function. \n",
    "A process named gradient descent optimization is a common method for finding the (local) minimum of a function by taking iterative steps along the gradient in a negative (descending) direction. \n",
    "There are several choices of gradient descent optimization algorithms already implemented in TensorFlow, and in this tutorial we will be using the Adam optimizer. \n",
    "This extends upon gradient descent optimization by using momentum to speed up the process through computing an exponentially weighted average of the gradients and using that in the adjustments.\"\"\"\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialzie all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 1   Train Accuracy= 0.8868545   Test Accuracy= 0.8828\n",
      "Epoch= 11   Train Accuracy= 0.97294545   Test Accuracy= 0.9509\n",
      "Epoch= 21   Train Accuracy= 0.98065454   Test Accuracy= 0.9562\n",
      "Epoch= 31   Train Accuracy= 0.983   Test Accuracy= 0.9552\n",
      "Epoch= 41   Train Accuracy= 0.96534544   Test Accuracy= 0.9469\n",
      "Epoch= 51   Train Accuracy= 0.97005457   Test Accuracy= 0.9506\n",
      "Epoch= 61   Train Accuracy= 0.9517091   Test Accuracy= 0.9396\n",
      "Epoch= 71   Train Accuracy= 0.9556   Test Accuracy= 0.9462\n",
      "Epoch= 81   Train Accuracy= 0.9762727   Test Accuracy= 0.9589\n",
      "Epoch= 91   Train Accuracy= 0.9679818   Test Accuracy= 0.9525\n",
      "Optimizaton Finished for all epochs!\n",
      "Epoch= 100 Train Accuracy= 0.9774 Test Accuracy= 0.9615\n"
     ]
    }
   ],
   "source": [
    "# run the graph within session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # loop through all the epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        #get total batches and loop through them\n",
    "        total_batches = int(mnist.train.num_examples / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            \n",
    "            # get trainX and trainY for each batch from training data\n",
    "            trainX, trainY = mnist.train.next_batch(batch)\n",
    "            \n",
    "            # Train & run optimzer, cost in same session and feed values to Variables\n",
    "            o, c = sess.run([optimizer, cost], feed_dict={X: trainX, Y:trainY})\n",
    "            \n",
    "            #compute average cost\n",
    "            avg_cost += c / total_batches\n",
    "            \n",
    "            if (epoch % display_step) == -1:\n",
    "                print ('batch=', batch+1, 'Cost=', c, 'Average Cost=', avg_cost, ' for Total batches', total_batches)\n",
    "        \n",
    "        # Test Model for every epoch\n",
    "        correct_predection = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "        \n",
    "        # Calculate Accuracy aftering training\n",
    "        accuracy_train = tf.reduce_mean(tf.cast(correct_predection, tf.float32))\n",
    "        accuracy_eval_train = accuracy_train.eval({X: mnist.train.images, Y: mnist.train.labels})\n",
    "        \n",
    "        # Calculate Accuracy aftering testing\n",
    "        accuracy_test = tf.reduce_mean(tf.cast(correct_predection, tf.float32))\n",
    "        accuracy_eval_test = accuracy_test.eval({X: mnist.test.images, Y: mnist.test.labels})\n",
    "        \n",
    "        # print display step with epoch, and cost\n",
    "        if (epoch % display_step) == 0 :\n",
    "            print ('Epoch=', epoch+1, '  Train Accuracy=', accuracy_eval_train,'  Test Accuracy=', accuracy_eval_test)\n",
    "        \n",
    "    print(\"Optimizaton Finished for all epochs!\")\n",
    "    print ('Epoch=', epoch+1, 'Train Accuracy=', accuracy_eval_train, 'Test Accuracy=', accuracy_eval_test)\n",
    "    # print optimization finished for all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
